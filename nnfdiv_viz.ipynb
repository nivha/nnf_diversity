{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0151a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import torch\n",
    "import torchvision.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc942c6",
   "metadata": {},
   "source": [
    "### Load videos and compute NNFDIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cda30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.video import read_original_video\n",
    "from nnf_evaluation import resize_by_target_size, get_nnfdiv\n",
    "\n",
    "vid_orig = read_original_video('./data/airbaloons/original', start_frame=0, end_frame=74)\n",
    "vid_orig = resize_by_target_size(vid_orig, target_shape=144)\n",
    "\n",
    "vid_vgpnn = read_original_video('./data/airbaloons/vgpnn/1', start_frame=0, end_frame=74)\n",
    "vid_vgpnn = resize_by_target_size(vid_vgpnn, target_shape=144)\n",
    "\n",
    "vid_sinfusion = read_original_video('./data/airbaloons/sinfusion/1', start_frame=0, end_frame=74)\n",
    "vid_sinfusion = resize_by_target_size(vid_sinfusion, target_shape=144)\n",
    "\n",
    "nnf_vgpnn, nnfdiv = get_nnfdiv(vid_vgpnn, vid_orig)\n",
    "print('NNFDIV VGPNN:', nnfdiv)\n",
    "nnf_sinfusion, nnfdiv = get_nnfdiv(vid_sinfusion, vid_orig)\n",
    "print('NNFDIV SinFusion:', nnfdiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a69d07",
   "metadata": {},
   "source": [
    "### Visualizing the NNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96439d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from torchvision.transforms.functional import equalize\n",
    "from common_utils.video import html_vid\n",
    "\n",
    "def nnf2rgb(nnf):\n",
    "    \"\"\" \n",
    "        @param nnf of shape: 1CTHW\n",
    "        Do histogram equalization on the NNF (for visualization purposes only..) \n",
    "        and convert to numpy video.\n",
    "    \"\"\"\n",
    "    n,c,t,h,w = nnf.shape\n",
    "    # equalize NNF\n",
    "    nnf_eq = equalize(nnf.contiguous().to(torch.uint8).view(n,c,t*h,w)).view(n,c,t,h,w)\n",
    "    # return as uint8 with shape THWC (which is the usual format for numpy videos)\n",
    "    return nnf_eq[0].permute(1,2,3,0).contiguous().to('cpu', torch.uint8)\n",
    "\n",
    "nnf_rgb = nnf2rgb(nnf_vgpnn)\n",
    "HTML(html_vid(nnf_rgb).to_html5_video())\n",
    "\n",
    "# run code below to save video (need to install \"av\"):\n",
    "# torchvision.io.write_video('./nnf_vid.mp4', nnf_rgb, fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143895e",
   "metadata": {},
   "source": [
    "#### Code for reproducing the NNF maps in [SinFusion Project Page](https://yaniv.nikankin.com/sinfusion/static/experiments.html)\n",
    "As opposed to the NNF map viz above, The NNF maps in the SinFusion project page:\n",
    "1. Only show the NNF in the \"TH plane\".\n",
    "2. Show the [\"flow wheel\"](https://people.csail.mit.edu/celiu/OpticalFlow/) colors with flow-viz (need to [pip install flow_viz](https://pypi.org/project/flow-vis/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from tqdm.auto import tqdm\n",
    "from common_utils.video import html_vid\n",
    "import flow_vis\n",
    "\n",
    "def nnf_th2rgb(nnf):\n",
    "    flows = []\n",
    "    for t in tqdm(range(nnf.shape[2])):\n",
    "        nnf_th = nnf[0, [1,2], t, :, :]\n",
    "        flowviz_x = flow_vis.flow_to_color(nnf_th.permute(1,2,0).cpu().numpy(), convert_to_bgr=False)\n",
    "        flows.append(torch.from_numpy(flowviz_x))\n",
    "    return torch.stack(flows)\n",
    "\n",
    "torchvision.io.write_video('./NNF_TH_vgpnn.mp4', nnf_th2rgb(nnf_vgpnn), fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87034d1a",
   "metadata": {},
   "source": [
    "### Computing the NNFDIV from an NNF\n",
    "\n",
    "This is pretty straightforward - it is simply the compression ratio of the NNF (which gives an upper bound on the minimal description length of the flow field).\n",
    "\n",
    "The idea is that this penalizes \"simple\" generated samples, that are just some \"shuffling\" of the original image or video (which is the case in [VGPNN](https://nivha.github.io/vgpnn), [GPNN](https://www.wisdom.weizmann.ac.il/~vision/gpnn) and [SinGAN](https://arxiv.org/abs/1905.01164) or other methods for generation from a single image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5768e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "def zlib_score(nnf):\n",
    "    x = nnf.to('cpu', torch.uint8).contiguous().numpy()\n",
    "    return len(zlib.compress(x)) / x.size\n",
    "\n",
    "zlib_score(nnf_vgpnn), zlib_score(nnf_sinfusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.io.write_video('./vid_orig.mp4', vid_orig[0].add(1).div(2).clip(0,1).mul(255).permute(1,2,3,0).contiguous().to('cpu', torch.uint8), fps=15)\n",
    "torchvision.io.write_video('./vid_vgpnn.mp4', vid_vgpnn[0].add(1).div(2).clip(0,1).mul(255).permute(1,2,3,0).contiguous().to('cpu', torch.uint8), fps=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
